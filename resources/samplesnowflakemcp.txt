Introduction

This document outlines the technical design for a Model Context Protocol (MCP) server that provides secure, governed access to Snowflake data warehouse capabilities within our banking environment. The Snowflake Managed  MCP Server enables AI assistants and agentic workflows to execute SQL queries, retrieve schema information, and interact with banking data while maintaining strict compliance with regulatory requirements including SOX, GLBA, PCI-DSS, and internal data governance policies.
The solution implements a defense-in-depth security model with multiple layers of authentication, authorization, query validation, and audit logging. All database interactions are mediated through the MCP server, which enforces role-based access control (RBAC), implements query allow-lists and deny-lists, and provides comprehensive audit trails for regulatory compliance.
Key capabilities include: secure query execution with parameterized queries to prevent SQL injection, schema exploration tools for AI assistants to understand data structures, result set handling with automatic PII detection and masking, and integration with the bank's existing identity and access management (IAM) infrastructure.


Model Context Protocol Overview
The Model Context Protocol (MCP) is an open standard developed by Anthropic that enables secure, standardized communication between AI applications and external data sources or tools. MCP follows a client-server architecture where MCP clients (typically AI assistants or agentic frameworks) connect to MCP servers that expose specific capabilities through a well-defined interface.
MCP servers expose three primary primitive types: Tools (executable functions that perform actions), Resources (data sources that can be read), and Prompts (templated instructions for common workflows). This design document focuses on implementing an MCP server that exposes Snowflake capabilities through these primitives while maintaining banking-grade security controls.

Snowflake provides a fully managed MCP server that lets AI agents securely retrieve data from Snowflake accounts without needing to deploy separate infrastructure. The managed server supports Cortex Analyst, Cortex Search, Cortex Agents, SQL execution, and custom tools (UDFs/stored procedures) through a standardized MCP interface with OAuth 2.0 authentication and role-based access control.

The banking industry is increasingly adopting AI-powered solutions for data analysis, risk assessment, fraud detection, and customer service enhancement. However, direct AI access to production databases presents significant security and compliance risks. The MCP Snowflake Server addresses these challenges by providing a controlled gateway that enables AI capabilities while enforcing appropriate guardrails.
Primary business drivers include:
-	Enable AI-assisted data analysis for business analysts and data scientists without requiring direct database credentials
-	Provide a governed interface for agentic AI workflows that need to query banking data
-	Maintain comprehensive audit trails for all AI-initiated database interactions
-	Reduce risk of data breaches through centralized access control and query validation
-	Support regulatory compliance requirements for AI governance

The combination of MCP standardization with Snowflake Cortex AI capabilities enables powerful use cases:
-	Self-service analytics: Business users can query complex data using natural language through Cortex Analyst
-	Document intelligence: RAG-powered search over compliance documents, policies, and regulatory filings via Cortex Search
-	Automated agents: Cortex Agents that combine structured data queries with unstructured document search for comprehensive analysis
-	Governance compliance: All AI operations remain within Snowflake's governance boundary with full audit trails


In Scope:
-	Read-only SQL query execution against approved Snowflake databases
-	Cortex Analyst integration with semantic views for text-to-SQL
-	Cortex Search integration for RAG over banking documents
-	Cortex Agent orchestration for multi-step analytical workflows
-	Custom tool integration via UDFs and stored procedures
-	Schema introspection and metadata retrieval
-	Integration with enterprise SSO via OAuth 2.0
-	PII detection and dynamic data masking
Out of Scope (Current phase - can change):
-	Write operations (INSERT, UPDATE, DELETE)
-	DDL operations (CREATE, ALTER, DROP)
-	Stored procedure execution
-	Cross-database federation

High-Level Architecture
The MCP Snowflake Server operates as a stateless microservice that mediates all interactions between MCP clients and the Snowflake data warehouse. The architecture follows a layered design pattern with clear separation of concerns between transport, protocol handling, business logic, and data access layers.
The server supports two primary transport mechanisms: stdio (for local development and desktop AI applications) and Server-Sent Events over HTTP (SSE) for production deployments. Both transports implement the same MCP protocol semantics, ensuring consistent behavior regardless of deployment model.
 <Diagram  Here>
 
 
Transport Layer
The transport layer handles the low-level communication between MCP clients and the server. For stdio transport, the server reads JSON-RPC messages from standard input and writes responses to standard output, making it suitable for process-based integration with desktop applications. For HTTP+SSE transport, the server exposes REST endpoints for capability negotiation and uses Server-Sent Events for streaming responses, enabling deployment as a standalone service.

Protocol Handler
The protocol handler implements the MCP specification using JSON-RPC 2.0 message format. It handles message framing, request/response correlation, capability negotiation, and error handling. The handler validates all incoming messages against the MCP schema and rejects malformed requests before they reach the business logic layer.

Authentication and Authorization Module
This critical security component validates client identity and enforces access policies. It integrates with the bank's existing IAM infrastructure through OIDC/OAuth 2.0 for user authentication and supports service account authentication via mTLS certificates for automated workflows. The module implements fine-grained authorization based on the requesting user's roles, the target database/schema, and the operation type.

Query Validator and Sanitizer
Before any SQL reaches Snowflake, it passes through the query validator which performs static analysis to detect potentially dangerous operations. The validator enforces query allow-lists (permitted query patterns), deny-lists (blocked operations like DDL), complexity limits (join depth, result set size), and schema access restrictions. The sanitizer ensures all queries use parameterized statements to prevent SQL injection attacks.

Business Logic Layer
The business logic layer implements the MCP primitives (Tools, Resources, Prompts) specific to Snowflake operations. It handles query execution, result transformation, schema discovery, and caching. This layer also implements data masking logic, applying redaction rules to sensitive fields in query results before returning them to clients.

Data Access Layer
The data access layer manages the connection pool to Snowflake, handles connection lifecycle, implements retry logic with exponential backoff, and manages query timeouts. It uses the official Snowflake connector with TLS 1.3 encryption for all communications. Connection credentials are retrieved from the bank's secrets management system (HashiCorp Vault or AWS Secrets Manager) at runtime.

Audit and Telemetry
Every operation is logged with sufficient detail for security analysis and compliance reporting. Audit logs capture the requesting user identity, timestamp, operation type, SQL executed (with parameters), result metadata, and execution duration. Logs are shipped to the bank's centralized SIEM platform for real-time monitoring and long-term retention. Telemetry data feeds into observability dashboards for operational monitoring.

Security Architecture
Security is the paramount concern for any system handling banking data. The MCP Snowflake Server implements a defense-in-depth strategy with multiple overlapping security controls. A compromise of any single layer should not result in unauthorized data access.

Authentication
User Authentication
Human users authenticate through the bank's enterprise SSO using OIDC. The MCP server validates JWT tokens issued by the identity provider, verifying signature, expiration, audience claims, and required scopes. Token validation occurs on every request; there is no session persistence in the MCP server. The server extracts user identity, group memberships, and entitlements from the token claims for authorization decisions.

Service Account Authentication
Automated workflows and AI agents authenticate using X.509 certificates issued by the bank's internal PKI. Mutual TLS (mTLS) ensures both client and server authentication. Service account certificates include custom extensions that encode the service identity, authorized operations, and data access permissions. Certificates have short validity periods (30 days) and require automated rotation.

Snowflake Authentication
The Snowflake-managed MCP server uses OAuth 2.0 authentication aligned with the MCP protocol's authorization specification. A security integration must be created with OAUTH_CLIENT_TYPE = 'CONFIDENTIAL'. Dynamic client registration is not supported; clients must be pre-registered. Programmatic Access Tokens (PATs) are supported but should use least-privileged roles.
The MCP server authenticates to Snowflake using key-pair authentication with RSA keys. Private keys are stored in the secrets management system and loaded into memory only when needed. The server uses a dedicated Snowflake service account with minimal privileges (USAGE on specific warehouses and SELECT on approved schemas). Key rotation occurs every 90 days through automated processes.

Authorization
Authorization follows the principle of least privilege with multiple enforcement points. The system implements a layered authorization model

<Diagram and Table here>


Role-Based Access Control (RBAC)
Users are assigned roles that map to Snowflake access patterns. Roles are hierarchical and define which databases, schemas, and tables a user can query. The role definitions are managed through infrastructure-as-code and version controlled. Changes require approval from both the data governance team and security team.

Query-Level Authorization
Beyond role-based database access, the system enforces query-level policies. Each query is analyzed to extract the tables and columns referenced, then validated against the user's entitlements. Queries accessing unauthorized objects are rejected with a generic error message that does not reveal schema details. This prevents enumeration attacks where malicious queries probe for table existence.

Row-Level Security
For tables containing data segregated by business unit, region, or customer segment, the MCP server automatically injects row-level security predicates into queries. These predicates filter results based on the user's organizational assignment, ensuring users only see data they are authorized to access even when querying shared tables.

Query Security
SQL Injection Prevention
All queries must use parameterized statements. The MCP server never constructs SQL through string concatenation. User-provided values are always passed as bind parameters, preventing SQL injection regardless of the input content. The query validator additionally scans for common injection patterns and rejects suspicious queries.

Query Allow-List and Deny-List
The system maintains configurable allow-lists of permitted query patterns and deny-lists of prohibited operations. By default, only SELECT statements are permitted. DDL (CREATE, ALTER, DROP), DML (INSERT, UPDATE, DELETE), and administrative commands (GRANT, REVOKE) are blocked. The allow-list can be extended for specific use cases through a formal change request process.

Query Complexity Limits
To prevent denial-of-service through resource-intensive queries, the system enforces complexity limits including maximum query execution time (configurable, default 5 minutes), maximum result set size (default 10,000 rows), maximum join depth (default 5 tables), and prohibition of certain expensive operations (e.g., CROSS JOIN without explicit approval).

Data Protection
PII Detection and Masking
The MCP server integrates with the bank's data classification system to identify columns containing PII. Results are automatically processed through a masking engine that applies appropriate redaction based on the data type and user's authorization level. Common masking patterns include full redaction (replaced with 'REDACTED'), partial masking (showing last 4 digits of SSN), tokenization (consistent pseudonymous identifiers), and format-preserving encryption for data that must maintain structure.

Encryption
All data in transit is encrypted using TLS 1.3. The MCP server terminates TLS connections from clients and establishes separate TLS connections to Snowflake. Data at rest encryption is handled by Snowflake's native encryption capabilities. Sensitive configuration (credentials, certificates) is encrypted in the secrets management system using AES-256-GCM.

Audit and Compliance
Comprehensive audit logging captures every operation with sufficient detail for security investigations and compliance reporting. Audit records are immutable, cryptographically signed, and retained according to regulatory requirements (minimum 7 years for banking operations). The audit log schema includes fields for timestamp (UTC with microsecond precision), request ID (unique identifier for correlation), user identity (from authentication token), client IP address (and X-Forwarded-For chain), operation type (tool invocation, resource access), SQL executed (with bind parameters logged separately), execution duration, rows returned (count only, not data), result status (success/failure with error codes), and session context (user agent, MCP client version).

MCP Tools
The MCP server exposes the following tools for Snowflake interaction. Each tool includes comprehensive input validation, authorization checks, and audit logging.
1.	execute_query: Executes a read-only SQL query against an authorized database. Accepts parameters for the SQL statement (with placeholders), bind parameters (array of values), database/schema context, and optional result format (JSON, CSV, or Markdown table). Returns column metadata and row data with automatic PII masking applied.
2.	get_schema: Retrieves schema information for authorized databases, schemas, or tables. Returns database/schema names with descriptions, table names with row counts and descriptions, column names with data types, nullability, and descriptions, as well as primary key and foreign key relationships. Useful for AI assistants to understand data structure before writing queries.
3.	validate_query: Validates a SQL query without executing it. Performs syntax validation via SQL parser, authorization check against user's entitlements, complexity analysis and limit validation, and identification of tables/columns referenced. Returns validation status and any issues found. Allows AI assistants to check queries before execution.
4.	get_query_history: Retrieves the user's recent query history from the current session. Returns executed queries with execution time, row counts, and status. Useful for AI assistants to understand context of previous operations.
5.	explain_query: Returns the execution plan for a query without running it. Provides estimated cost, join strategy, and partitions scanned. Helps AI assistants optimize query performance before execution.
6.	cortex_analyst_query: Executes a natural language query against a semantic view using Cortex Analyst's text-to-SQL engine. Accepts parameters for the user message (natural language question), semantic view identifier (fully qualified name), and optional conversation history for multi-turn context. Returns the generated SQL statement, a natural language explanation of the query logic, and the query results with column metadata. Requires SELECT privilege on the target semantic view. Useful for enabling business users to query complex banking data without SQL knowledge.
7.	cortex_search_query: Performs hybrid vector and keyword search over a Cortex Search Service for RAG applications. Accepts parameters for the search query (natural language text), columns to return (array of column names), filter object (JSON filter criteria on attributes), and limit (maximum results, default 10). Returns ranked search results with relevance scores, matched text chunks, and associated metadata columns. Requires USAGE privilege on the Cortex Search Service. Useful for searching compliance documents, policies, regulatory filings, and unstructured banking records.
8.	cortex_agent_run: Invokes a Cortex Agent for multi-step reasoning and tool orchestration. Accepts parameters for the user message (task description or question) and optional session context for stateful interactions. Returns the agent's response including any intermediate tool invocations, generated SQL queries, search results, and final synthesized answer. Requires USAGE privilege on the Cortex Agent. Useful for complex analytical workflows that combine structured data queries with document search and custom business logic.
9.	invoke_custom_function: Executes a user-defined function (UDF) or stored procedure exposed as a GENERIC tool type. Accepts parameters defined by the function's input schema (varies per function), warehouse identifier for compute, and function type (function or procedure). Returns the function output in the declared return type (scalar, VARIANT/JSON, or ARRAY). Requires USAGE privilege on the UDF or stored procedure. Useful for invoking banking-specific calculations such as risk scoring, fraud detection models, or regulatory reporting functions.
10.	cortex_complete: Invokes Cortex LLM functions for text generation tasks. Accepts parameters for the prompt (text input), model selection (e.g., mistral-large, llama3-70b), max_tokens (output length limit), and temperature (creativity control 0-1). Returns the generated text completion. Useful for summarization, sentiment analysis, translation, and free-form text generation on banking data within query results.
11.	cortex_summarize: Generates a concise summary of input text using Cortex's SUMMARIZE function. Accepts parameters for the source text (document content or query results) and optional summary length preference (short, medium, long). Returns a condensed summary preserving key information. Useful for summarizing lengthy compliance documents, customer communications, or aggregated report data for executive consumption.
12. cortex_sentiment: Analyzes sentiment of text content using Cortex's SENTIMENT function. Accepts parameters for the input text (customer feedback, communications, or document excerpts). Returns a sentiment score (-1 to 1 scale) and categorical classification (positive, negative, neutral). Useful for analyzing customer complaint sentiment, monitoring social media mentions, or flagging negative communications for escalation.

Cortex AI Specific Tool types:
- CORTEX_SEARCH_SERVICE_QUERY: Cortex Search Service tool
- CORTEX_ANALYST_MESSAGE: Cortex Analyst tool
- SYSTEM_EXECUTE_SQL: SQL execution
- CORTEX_AGENT_RUN: Cortex Agent tool
- GENERIC: tool for UDFs and stored procedures


MCP Resources
Resources provide read-only access to data that AI assistants can reference. Unlike tools, resources are designed for data that changes infrequently and can be cached.
1.	snowflake://databases: Lists all databases the user is authorized to access, with metadata including description, creation date, and schema count.
2.	snowflake://{database}/schemas: Lists schemas within a database, including managed access status and table count.
3.	snowflake://{database}/{schema}/tables: Lists tables within a schema, including table type (BASE TABLE, VIEW, etc.), row count estimate, and size.
4.	snowflake://{database}/{schema}/{table}/columns: Lists columns for a table with data type, nullability, default value, and classification tags.
5.	snowflake://warehouses: Lists available compute warehouses with size, state, and current utilization.


MCP Prompts
Prompts provide templated instructions for common workflows. These help standardize AI assistant behavior for banking-specific use cases.
-	analyze_table: Guides the AI assistant through exploring a table's structure, understanding column semantics, and generating appropriate analysis queries.
-	data_quality_check: Template for common data quality validations including null checks, uniqueness verification, and referential integrity tests.
-	safe_query_pattern: Enforces best practices for query construction including proper filtering, result limiting, and avoiding common anti-patterns.

<Table here>


Snowflake Integration
Connection Management
The MCP server maintains a connection pool to Snowflake to optimize performance while controlling resource usage. Connection pooling configuration includes minimum connections (maintains baseline for responsiveness), maximum connections (prevents resource exhaustion), idle timeout (releases unused connections), and connection validation (periodic health checks).
Each connection is associated with the MCP server's service account. Per-user authorization is enforced at the MCP layer, not through separate Snowflake users. This approach simplifies credential management while maintaining audit traceability through query tags.

Query Execution Flow
The query execution process follows a strict sequence designed for security and observability:
1.	Request Reception: MCP server receives tool invocation with SQL and parameters
2.	Authentication Validation: Verify JWT/certificate and extract user identity
3.	Input Validation: Validate request structure against Zod schema
4.	SQL Parsing: Parse SQL to extract AST for analysis
5.	Authorization Check: Verify user can access referenced tables/columns
6.	Query Validation: Check against allow-list/deny-list and complexity limits
7.	Query Tagging: Inject query tags with request ID, user identity for traceability
8.	Execution: Submit parameterized query to Snowflake via connection pool
9.	Result Processing: Apply PII masking to result set
10.	Audit Logging: Record execution details to audit log
11.	Response: Return formatted results to MCP client

<Diagram here>

Error Handling
Error handling follows security best practices by providing helpful messages for legitimate errors while avoiding information disclosure that could aid attackers. Snowflake errors are categorized and mapped to appropriate MCP error responses with generic messages for security-sensitive failures (authorization, invalid table references) and specific messages for operational issues (syntax errors, timeout). Internal error details are logged but not exposed to clients.

<Table here>

Performance Optimization
Several strategies optimize query performance and reduce Snowflake compute costs:
-	Result Caching: Identical queries from the same user within a configurable window return cached results. Cache keys include query hash, user identity, and timestamp bucket.
-	Schema Caching: Database and schema metadata is cached with longer TTL since it changes infrequently. Cache invalidation occurs on schema change notifications or manual refresh.
-	Warehouse Routing: Queries are routed to appropriately-sized warehouses based on estimated complexity. Simple metadata queries use smaller warehouses; complex analytical queries can request larger resources.
-	Query Queuing: When concurrent query limits are reached, requests are queued with fair scheduling based on user priority tiers.

Sample Use cases (Cortex specific):
1.	Transaction Analytics: "What was our total transaction volume by region last quarter?"
2.	Risk Analysis: "Show me the top 10 accounts by overdraft frequency this month"
3.	Customer Insights: "What is our customer acquisition cost trend over the past year?"
4.	Compliance Reporting: "Generate a summary of suspicious activity reports by category"

Resource and Tool Discovery in the Model Context Protocol
Generic MCP Discovery
The Model Context Protocol establishes a dynamic discovery mechanism that enables AI agents to learn about server capabilities at runtime rather than requiring hardcoded knowledge of available functionality. When an MCP client connects to a server, it first performs an initialization handshake that negotiates the protocol version and returns a capabilities object describing what the server supports. This capabilities object indicates whether the server offers tools, resources, prompts, or other primitives, allowing the client to understand the full scope of available functionality before attempting any operations.
Following initialization, the client can invoke discovery methods corresponding to each supported capability. The tools/list method returns all available tools with their names, descriptions, and JSON Schema definitions for input parameters. Similarly, resources/list returns available data sources that can be read, and prompts/list returns templated workflows the server offers. Each discovery response provides sufficient metadata for an AI agent to understand not only what operations are available but exactly how to invoke them correctly, including required parameters, optional arguments, data types, and constraints.
The discovery mechanism is designed to be permission-aware, meaning clients only see capabilities they are authorized to use. This is particularly important in enterprise environments where different users or service accounts may have varying levels of access. A sales analyst connecting to the same MCP server as a compliance officer might see entirely different tools based on their respective entitlements. This dynamic, permission-filtered discovery enables organizations to deploy a single MCP server that serves multiple user populations with appropriate capability exposure for each.
MCP also supports change notifications through the listChanged capability, which allows servers to proactively inform clients when available tools, resources, or prompts have been modified. This is valuable in environments where server configurations evolve frequently, as clients can subscribe to changes rather than repeatedly polling the discovery endpoints. For servers that do not support change notifications, clients must implement their own refresh strategy to ensure they maintain an accurate understanding of available capabilities over time.
Snowflake MCP Server Discovery
The Snowflake-managed MCP server implements tool discovery aligned with the MCP specification but currently limits capabilities to tools only. When a client initializes a connection and calls tools/list, the server returns all configured tools from the MCP server object specification, including Cortex Analyst semantic views, Cortex Search services, Cortex Agents, SQL execution tools, and custom functions exposed as generic tools. Each tool definition includes the name used for invocation, a description that helps AI agents select the appropriate tool for a given user intent, and a complete input schema defining the parameters each tool accepts.
Authorization plays a critical role in Snowflake's discovery implementation. The tools returned by tools/list are filtered based on the authenticated user's Snowflake role and the specific privileges granted on underlying objects. A user must have USAGE privilege on the MCP server object to connect and discover tools, but visibility of individual tools additionally requires the appropriate privilege on each tool's underlying resource—SELECT on semantic views for Cortex Analyst tools, USAGE on Cortex Search services for search tools, and USAGE on UDFs or stored procedures for custom tools. This means two users connecting to the same MCP server may receive different tool lists based on their respective entitlements, ensuring that discovery itself does not leak information about capabilities users cannot access.
The current Snowflake implementation does not support resources, prompts, roots, or change notifications, and only provides non-streaming responses. This means clients cannot discover readable data sources through resources/list or templated workflows through prompts/list, and must rely exclusively on tool invocations for all Snowflake interactions. Additionally, because listChanged notifications are not supported, clients that need to detect configuration changes must periodically re-invoke tools/list to refresh their understanding of available capabilities. These limitations reflect the current state of the managed MCP server and are expected to evolve as Snowflake expands its MCP implementation.

Pagination:

Generic MCP Pagination
The Model Context Protocol supports pagination for discovery operations that may return large result sets, such as tools/list, resources/list, and prompts/list. When a server has more items than it wishes to return in a single response, it includes a nextCursor field in the result object. The client can then make subsequent requests with this cursor value in the request parameters to retrieve the next page of results. This cursor-based approach is preferred over offset-based pagination because it handles dynamic lists more gracefully—if items are added or removed between requests, cursor-based pagination maintains consistency without skipping or duplicating entries.
Pagination is optional and implementation-dependent, meaning servers may choose to return all results in a single response if the result set is small enough. Clients should be designed to handle both paginated and non-paginated responses by checking for the presence of the nextCursor field and continuing to request additional pages until no cursor is returned. For tool invocation results, pagination behavior depends on the specific tool implementation. Tools returning large data sets, such as database query results, may implement their own pagination through input parameters like limit and offset or by returning a cursor that can be passed to subsequent invocations. The MCP specification does not mandate a specific pagination approach for tool results, leaving this to the discretion of server and tool implementers based on their specific use cases.
Snowflake MCP Server Pagination
The Snowflake-managed MCP server currently returns non-streaming responses, meaning query results and tool outputs are delivered in a single response rather than streamed incrementally. For Cortex Search tools, pagination is handled through the limit parameter in the input schema, which controls the maximum number of search results returned in a single invocation. Clients needing to retrieve additional results beyond the specified limit must make subsequent calls with modified query parameters or filters to navigate through larger result sets.
For SQL execution and Cortex Analyst tools, result set size management is typically handled through SQL-level controls such as LIMIT and OFFSET clauses in the generated or provided queries, rather than through MCP-level pagination mechanisms. This approach aligns with standard database interaction patterns and gives clients explicit control over result set sizes. Given the banking context where queries may return substantial transaction histories or analytical datasets, implementing appropriate result limits is essential both for performance and for compliance with data minimization principles that restrict unnecessary bulk data retrieval.

Stdio transport
The stdio transport offers several advantages for local development and single-user scenarios. It requires no network configuration, firewall rules, or TLS certificate management since communication occurs entirely within the local system through operating system pipes. Startup is straightforward—the client simply spawns the server process and begins exchanging messages. This makes stdio ideal for personal productivity tools, local development environments, and scenarios where the MCP server accesses local resources like file systems or desktop applications. However, stdio is inherently limited to single-client scenarios since only one process can control the server's standard streams, and it cannot be used for remote or multi-tenant deployments where multiple clients need concurrent access to shared server capabilities.



